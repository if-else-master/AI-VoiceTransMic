import pyaudio
import wave
import threading
import time
import queue
from collections import deque
import numpy as np
import torch
import scipy.io.wavfile
import tempfile
import os
import pygame
import google.generativeai as genai
from OpenVoice.checkpoints_v2.MeloTTS.melo.api import TTS
from OpenVoice.openvoice.api import ToneColorConverter
import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext
from datetime import datetime
import glob
import asyncio
import concurrent.futures

# åœ¨ç¨‹åºé–‹å§‹æ™‚å°±ç¦ç”¨MPSä»¥é¿å…è¨­å‚™åˆ†é…å•é¡Œ
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
if torch.backends.mps.is_available():
    torch.backends.mps.is_available = lambda: False
    print("âš ï¸ å·²å…¨å±€ç¦ç”¨MPSè¨­å‚™ä»¥é¿å…å…¼å®¹æ€§å•é¡Œ")

class RealtimeVoiceTranslator:
    def __init__(self):
        # ç³»çµ±é…ç½®
        # å¼·åˆ¶ä½¿ç”¨CPUé¿å…MacOS MPSè¨­å‚™å•é¡Œ
        self.device = "cpu"
        self.rate = 16000
        self.chunk = 512  # æ›´å°çš„chunkä»¥æ¸›å°‘å»¶é²
        self.format = pyaudio.paInt16
        self.channels = 1
        
        # å³æ™‚è™•ç†åƒæ•¸ - æ¥µç«¯å„ªåŒ–ç‚º3ç§’ä»¥å…§
        self.silence_threshold = 200  # é€²ä¸€æ­¥é™ä½é–¾å€¼
        self.silence_duration = 0.2   # ç¸®çŸ­åˆ°0.2ç§’
        self.min_speech_duration = 0.1  # æœ€çŸ­èªéŸ³é•·åº¦
        self.max_segment_duration = 2.0  # æ›´çŸ­çš„èªéŸ³æ®µ
        
        # ç³»çµ±ç‹€æ…‹
        self.is_active = False
        self.should_stop = False
        self.gemini_api_key = None
        self.model = None
        
        # OpenVoiceæ¨¡å‹
        self.tone_color_converter = None
        self.tts_models = {}
        self.target_se = None
        self.cloned_voice_path = None
        
        # èªè¨€è¨­ç½®
        self.source_language = 'zh'
        self.target_language = 'en'
        self.supported_languages = {
            'zh': 'ä¸­æ–‡', 'en': 'è‹±æ–‡', 'ja': 'æ—¥æ–‡', 'ko': 'éŸ“æ–‡',
            'es': 'è¥¿ç­ç‰™æ–‡', 'fr': 'æ³•æ–‡'
        }
        self.openvoice_language_map = {
            'zh': 'ZH', 'en': 'EN_NEWEST', 'ja': 'JP', 
            'ko': 'KR', 'es': 'ES', 'fr': 'FR'
        }
        
        # è™•ç†éšŠåˆ— - ä½¿ç”¨æ›´å°çš„éšŠåˆ—ä»¥æ¸›å°‘å»¶é²
        self.audio_queue = queue.Queue(maxsize=5)
        self.transcription_queue = queue.Queue(maxsize=3)
        self.synthesis_queue = queue.Queue(maxsize=3)
        self.playback_queue = queue.Queue(maxsize=2)
        
        # çµ±è¨ˆä¿¡æ¯
        self.processing_times = deque(maxlen=10)
        
        # GUI
        self.gui = None
        
        # åŸ·è¡Œç·’æ± 
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)
        
        # åˆå§‹åŒ–pygame
        pygame.mixer.init(frequency=24000, size=-16, channels=1, buffer=512)
        
        print("ğŸš€ è¶…ä½å»¶é²å³æ™‚èªéŸ³ç¿»è­¯ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    
    def setup_api(self, api_key):
        """è¨­ç½®API"""
        try:
            self.gemini_api_key = api_key
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel('gemini-2.0-flash-exp')
            # æ¸¬è©¦API
            test_response = self.model.generate_content("test")
            print("âœ… Gemini API è¨­ç½®æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ API è¨­ç½®å¤±æ•—: {e}")
            return False
    
    def load_models(self):
        """é è¼‰å…¥æ‰€æœ‰æ¨¡å‹ä»¥æ¸›å°‘å»¶é²"""
        try:
            print("ğŸ”„ é è¼‰å…¥æ¨¡å‹ä¸­...")
            start_time = time.time()
            
            # è¼‰å…¥ToneColorConverter
            ckpt_converter = 'OpenVoice/checkpoints_v2/converter'
            self.tone_color_converter = ToneColorConverter(
                f'{ckpt_converter}/config.json', 
                device=self.device
            )
            self.tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')
            
            # é è¼‰å…¥å¸¸ç”¨èªè¨€çš„TTSæ¨¡å‹
            for lang in ['EN_NEWEST', 'ZH', 'JP']:
                try:
                    self.tts_models[lang] = TTS(language=lang, device=self.device)
                    print(f"âœ… {lang} TTSæ¨¡å‹è¼‰å…¥å®Œæˆ")
                except Exception as e:
                    print(f"âš ï¸ {lang} TTSæ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            
            # å¦‚æœæœ‰ç¾å­˜èªéŸ³æ–‡ä»¶ï¼Œè¼‰å…¥ç‰¹å¾µ
            if os.path.exists("cloned_voices"):
                voice_files = glob.glob("cloned_voices/*.wav")
                if voice_files:
                    self.cloned_voice_path = voice_files[0]
                    try:
                        self.target_se = self.tone_color_converter.extract_se(self.cloned_voice_path)
                        # å¼·åˆ¶å°‡ç‰¹å¾µç§»åˆ°CPU
                        if hasattr(self.target_se, 'cpu'):
                            self.target_se = self.target_se.cpu()
                        print("âœ… èªéŸ³ç‰¹å¾µè¼‰å…¥å®Œæˆ")
                    except Exception as e:
                        print(f"âš ï¸ èªéŸ³ç‰¹å¾µè¼‰å…¥éŒ¯èª¤: {e}")
            
            load_time = time.time() - start_time
            print(f"âœ… æ‰€æœ‰æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œè€—æ™‚: {load_time:.2f}ç§’")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            return False
    
    def clone_voice(self, duration=3):
        """å¿«é€ŸèªéŸ³å…‹éš†"""
        try:
            print("ğŸ¤ é–‹å§‹å¿«é€ŸèªéŸ³éŒ„è£½...")
            
            if not os.path.exists("cloned_voices"):
                os.makedirs("cloned_voices")
            
            # éŒ„éŸ³
            audio = pyaudio.PyAudio()
            stream = audio.open(
                format=self.format,
                channels=self.channels,
                rate=self.rate,
                input=True,
                frames_per_buffer=self.chunk
            )
            
            frames = []
            start_time = time.time()
            
            print("ğŸ™ï¸ è«‹é–‹å§‹èªªè©±...")
            while time.time() - start_time < duration:
                data = stream.read(self.chunk)
                frames.append(data)
            
            stream.stop_stream()
            stream.close()
            audio.terminate()
            
            # ä¿å­˜éŒ„éŸ³
            clone_file = f"cloned_voices/voice_clone_{int(time.time())}.wav"
            wf = wave.open(clone_file, 'wb')
            wf.setnchannels(self.channels)
            wf.setsampwidth(pyaudio.PyAudio().get_sample_size(self.format))
            wf.setframerate(self.rate)
            wf.writeframes(b''.join(frames))
            wf.close()
            
            # æå–ç‰¹å¾µ
            if self.tone_color_converter:
                try:
                    # ç¢ºä¿ç‰¹å¾µæå–åœ¨CPUä¸Šé€²è¡Œ
                    self.target_se = self.tone_color_converter.extract_se(clone_file)
                    # å¼·åˆ¶å°‡ç‰¹å¾µç§»åˆ°CPU
                    if hasattr(self.target_se, 'cpu'):
                        self.target_se = self.target_se.cpu()
                    self.cloned_voice_path = clone_file
                    print("âœ… èªéŸ³å…‹éš†å®Œæˆ")
                    return True
                except Exception as e:
                    print(f"âš ï¸ èªéŸ³ç‰¹å¾µæå–éŒ¯èª¤: {e}")
                    return False
            
        except Exception as e:
            print(f"âŒ èªéŸ³å…‹éš†å¤±æ•—: {e}")
            return False
    
    def start_realtime_translation(self):
        """å•Ÿå‹•è¶…ä½å»¶é²å³æ™‚ç¿»è­¯"""
        if not self.model or not self.tone_color_converter:
            print("âŒ è«‹å…ˆè¨­ç½®APIå’Œè¼‰å…¥æ¨¡å‹")
            return False
        
        self.is_active = True
        self.should_stop = False
        
        # æ¸…ç©ºéšŠåˆ—
        self._clear_queues()
        
        # å•Ÿå‹•è™•ç†ç·šç¨‹
        threads = [
            threading.Thread(target=self._audio_capture_worker, daemon=True),
            threading.Thread(target=self._transcription_worker, daemon=True),
            threading.Thread(target=self._synthesis_worker, daemon=True),
            threading.Thread(target=self._playback_worker, daemon=True),
        ]
        
        for thread in threads:
            thread.start()
        
        self.threads = threads
        print("ğŸ¤ å³æ™‚ç¿»è­¯å·²å•Ÿå‹• - ç›®æ¨™å»¶é² < 3ç§’")
        return True
    
    def stop_realtime_translation(self):
        """åœæ­¢å³æ™‚ç¿»è­¯"""
        self.should_stop = True
        self.is_active = False
        
        # ç­‰å¾…ç·šç¨‹çµæŸ
        if hasattr(self, 'threads'):
            for thread in self.threads:
                thread.join(timeout=2)
        
        # æ¸…ç©ºéšŠåˆ—
        self._clear_queues()
        print("â¹ï¸ å³æ™‚ç¿»è­¯å·²åœæ­¢")
    
    def _clear_queues(self):
        """æ¸…ç©ºæ‰€æœ‰éšŠåˆ—"""
        queues = [self.audio_queue, self.transcription_queue, 
                 self.synthesis_queue, self.playback_queue]
        for q in queues:
            while not q.empty():
                try:
                    q.get_nowait()
                except queue.Empty:
                    break
    
    def _audio_capture_worker(self):
        """éŸ³é »æ•ç²ç·šç¨‹ - å„ªåŒ–ç‚ºæµå¼è™•ç†"""
        audio = pyaudio.PyAudio()
        stream = audio.open(
            format=self.format,
            channels=self.channels,
            rate=self.rate,
            input=True,
            frames_per_buffer=self.chunk
        )
        
        current_segment = []
        last_speech_time = 0
        is_speech_detected = False
        segment_start_time = time.time()
        
        print("ğŸ¤ éŸ³é »æ•ç²å•Ÿå‹•")
        
        while self.is_active and not self.should_stop:
            try:
                data = stream.read(self.chunk, exception_on_overflow=False)
                audio_np = np.frombuffer(data, dtype=np.int16)
                
                # è¨ˆç®—éŸ³é‡
                rms = np.sqrt(np.mean(audio_np.astype(np.float64)**2))
                current_time = time.time()
                
                # èªéŸ³æ´»å‹•æª¢æ¸¬
                if rms > self.silence_threshold:
                    if not is_speech_detected:
                        is_speech_detected = True
                        segment_start_time = current_time
                        if self.gui:
                            self.gui.update_status("ğŸ¤ æª¢æ¸¬åˆ°èªéŸ³...")
                    
                    current_segment.extend(audio_np)
                    last_speech_time = current_time
                
                else:
                    if is_speech_detected:
                        silence_duration = current_time - last_speech_time
                        segment_duration = current_time - segment_start_time
                        
                        # æ¢ä»¶åˆ¤æ–·ï¼šéœéŸ³è¶…æ™‚ æˆ– èªéŸ³æ®µéé•·
                        should_process = (
                            silence_duration >= self.silence_duration or 
                            segment_duration >= self.max_segment_duration
                        )
                        
                        if should_process and len(current_segment) > int(self.rate * self.min_speech_duration):
                            # ç™¼é€éŸ³é »æ®µè™•ç†
                            audio_data = np.array(current_segment, dtype=np.int16)
                            try:
                                self.audio_queue.put_nowait((audio_data, time.time()))
                                if self.gui:
                                    self.gui.update_status("ğŸ”„ è™•ç†èªéŸ³...")
                            except queue.Full:
                                print("âš ï¸ éŸ³é »éšŠåˆ—æ»¿ï¼Œè·³éæ­¤æ®µ")
                            
                            current_segment = []
                            is_speech_detected = False
                
            except Exception as e:
                print(f"âŒ éŸ³é »æ•ç²éŒ¯èª¤: {e}")
                break
        
        stream.stop_stream()
        stream.close()
        audio.terminate()
        print("ğŸ¤ éŸ³é »æ•ç²åœæ­¢")
    
    def _transcription_worker(self):
        """è½‰éŒ„å’Œç¿»è­¯ç·šç¨‹ - åˆä½µè™•ç†ä»¥æ¸›å°‘å»¶é²"""
        print("ğŸ“ è½‰éŒ„ç¿»è­¯ç·šç¨‹å•Ÿå‹•")
        
        while self.is_active or not self.audio_queue.empty():
            try:
                audio_data, capture_time = self.audio_queue.get(timeout=1)
                process_start = time.time()
                
                # ä¿å­˜è‡¨æ™‚éŸ³é »æ–‡ä»¶
                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
                scipy.io.wavfile.write(temp_file.name, self.rate, audio_data)
                
                # ä¸Šå‚³éŸ³é »åˆ°Gemini
                audio_file = genai.upload_file(path=temp_file.name)
                
                # åˆä½µè½‰éŒ„å’Œç¿»è­¯ç‚ºä¸€æ¬¡APIèª¿ç”¨ä»¥æ¸›å°‘å»¶é²
                source_lang = self.supported_languages[self.source_language]
                target_lang = self.supported_languages[self.target_language]
                
                if self.source_language == self.target_language:
                    prompt = f"è«‹å°‡é€™æ®µéŸ³é »ä¸­çš„{source_lang}èªéŸ³å…§å®¹è½‰æ›ç‚ºæ–‡å­—ã€‚åªå›å‚³è½‰éŒ„çš„æ–‡å­—å…§å®¹ã€‚"
                    response = self.model.generate_content([audio_file, prompt])
                    original_text = response.text.strip()
                    translated_text = original_text
                else:
                    # ä¸€æ¬¡æ€§å®Œæˆè½‰éŒ„å’Œç¿»è­¯
                    prompt = f"""è«‹åŸ·è¡Œä»¥ä¸‹å…©å€‹æ­¥é©Ÿï¼š
1. å°‡éŸ³é »ä¸­çš„{source_lang}èªéŸ³è½‰æ›ç‚ºæ–‡å­—
2. å°‡è½‰éŒ„çµæœç¿»è­¯ç‚º{target_lang}

è«‹ç”¨ä»¥ä¸‹æ ¼å¼å›æ‡‰ï¼š
åŸæ–‡ï¼š[è½‰éŒ„çµæœ]
ç¿»è­¯ï¼š[ç¿»è­¯çµæœ]"""
                    
                    response = self.model.generate_content([audio_file, prompt])
                    result = response.text.strip()
                    
                    # è§£æçµæœ
                    lines = result.split('\n')
                    original_text = ""
                    translated_text = ""
                    
                    for line in lines:
                        if line.startswith('åŸæ–‡ï¼š'):
                            original_text = line[3:].strip()
                        elif line.startswith('ç¿»è­¯ï¼š'):
                            translated_text = line[3:].strip()
                    
                    # å¦‚æœè§£æå¤±æ•—ï¼Œä½¿ç”¨æ•´å€‹å›æ‡‰ä½œç‚ºç¿»è­¯
                    if not translated_text:
                        translated_text = result
                        original_text = "è§£æå¤±æ•—"
                
                # æ¸…ç†æ–‡ä»¶
                genai.delete_file(audio_file.name)
                os.unlink(temp_file.name)
                
                process_time = time.time() - process_start
                total_latency = time.time() - capture_time
                
                print(f"ğŸ“ è½‰éŒ„: {original_text}")
                print(f"ğŸŒ ç¿»è­¯: {translated_text}")
                print(f"â±ï¸ è™•ç†æ™‚é–“: {process_time:.2f}s, ç¸½å»¶é²: {total_latency:.2f}s")
                
                # æ›´æ–°GUI
                if self.gui:
                    self.gui.add_text(original_text, translated_text)
                
                # ç™¼é€åˆ°èªéŸ³åˆæˆ
                if translated_text.strip():
                    try:
                        self.transcription_queue.put_nowait((translated_text, capture_time))
                    except queue.Full:
                        print("âš ï¸ è½‰éŒ„éšŠåˆ—æ»¿ï¼Œè·³éåˆæˆ")
                
                self.audio_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ è½‰éŒ„ç¿»è­¯éŒ¯èª¤: {e}")
        
        print("ğŸ“ è½‰éŒ„ç¿»è­¯ç·šç¨‹åœæ­¢")
    
    def _synthesis_worker(self):
        """èªéŸ³åˆæˆç·šç¨‹ - ä½¿ç”¨é è¼‰å…¥æ¨¡å‹"""
        print("ğŸ”Š èªéŸ³åˆæˆç·šç¨‹å•Ÿå‹•")
        
        while self.is_active or not self.transcription_queue.empty():
            try:
                text, capture_time = self.transcription_queue.get(timeout=1)
                
                if self.target_se is None:
                    print("âš ï¸ ç„¡èªéŸ³ç‰¹å¾µï¼Œè·³éåˆæˆ")
                    continue
                
                synthesis_start = time.time()
                
                # ç²å–ç›®æ¨™èªè¨€çš„TTSæ¨¡å‹
                target_lang_key = self.openvoice_language_map.get(self.target_language, 'EN_NEWEST')
                
                if target_lang_key not in self.tts_models:
                    print(f"âš ï¸ {target_lang_key} TTSæ¨¡å‹æœªè¼‰å…¥")
                    continue
                
                tts_model = self.tts_models[target_lang_key]
                
                # ç”ŸæˆåŸºç¤èªéŸ³
                temp_src = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
                speaker_ids = tts_model.hps.data.spk2id
                speaker_key = list(speaker_ids.keys())[0]
                speaker_id = speaker_ids[speaker_key]
                
                # ä½¿ç”¨æ›´å¿«çš„è¨­ç½®ï¼Œç¢ºä¿åœ¨CPUä¸ŠåŸ·è¡Œ
                try:
                    # ç¢ºä¿æ¨¡å‹åœ¨CPUä¸Š
                    if hasattr(tts_model, 'model'):
                        tts_model.model = tts_model.model.cpu()
                    
                    tts_model.tts_to_file(text, speaker_id, temp_src.name, speed=1.1, quiet=True)
                except Exception as e:
                    print(f"âš ï¸ TTSåˆæˆéŒ¯èª¤: {e}")
                    # å˜—è©¦é‡æ–°è¼‰å…¥æ¨¡å‹ä¸¦å¼·åˆ¶ä½¿ç”¨CPU
                    try:
                        del self.tts_models[target_lang_key]
                        self.tts_models[target_lang_key] = TTS(language=target_lang_key, device="cpu")
                        tts_model = self.tts_models[target_lang_key]
                        tts_model.tts_to_file(text, speaker_id, temp_src.name, speed=1.1, quiet=True)
                        print("âœ… é‡æ–°è¼‰å…¥TTSæ¨¡å‹æˆåŠŸ")
                    except Exception as e2:
                        print(f"âŒ é‡æ–°è¼‰å…¥TTSæ¨¡å‹å¤±æ•—: {e2}")
                        os.unlink(temp_src.name)
                        continue
                
                # èªéŸ³è½‰æ›
                speaker_key_formatted = speaker_key.lower().replace('_', '-')
                source_se_path = f'OpenVoice/checkpoints_v2/base_speakers/ses/{speaker_key_formatted}.pth'
                
                if os.path.exists(source_se_path):
                    # å¼·åˆ¶è¼‰å…¥åˆ°CPUä¸¦ç¢ºä¿å¼µé‡è¨­å‚™ä¸€è‡´
                    source_se = torch.load(source_se_path, map_location="cpu")
                    
                    # ç¢ºä¿target_seä¹Ÿåœ¨CPUä¸Š
                    if self.target_se is not None:
                        if hasattr(self.target_se, 'cpu'):
                            target_se_cpu = self.target_se.cpu()
                        else:
                            target_se_cpu = self.target_se
                    else:
                        target_se_cpu = self.target_se
                    
                    output_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
                    
                    try:
                        self.tone_color_converter.convert(
                            audio_src_path=temp_src.name,
                            src_se=source_se,
                            tgt_se=target_se_cpu,
                            output_path=output_file.name,
                            message="@RealTime"
                        )
                    except Exception as conv_error:
                        print(f"âš ï¸ èªéŸ³è½‰æ›éŒ¯èª¤: {conv_error}")
                        # å¦‚æœè½‰æ›å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹TTSè¼¸å‡º
                        import shutil
                        shutil.copy2(temp_src.name, output_file.name)
                    
                    synthesis_time = time.time() - synthesis_start
                    total_latency = time.time() - capture_time
                    
                    print(f"ğŸ”Š åˆæˆå®Œæˆï¼Œè€—æ™‚: {synthesis_time:.2f}s, ç¸½å»¶é²: {total_latency:.2f}s")
                    
                    # ç™¼é€åˆ°æ’­æ”¾éšŠåˆ—
                    try:
                        self.synthesis_queue.put_nowait((output_file.name, capture_time))
                    except queue.Full:
                        os.unlink(output_file.name)
                        print("âš ï¸ åˆæˆéšŠåˆ—æ»¿ï¼Œè·³éæ’­æ”¾")
                else:
                    print(f"âš ï¸ æºèªéŸ³ç‰¹å¾µæ–‡ä»¶ä¸å­˜åœ¨: {source_se_path}")
                
                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                os.unlink(temp_src.name)
                self.transcription_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ èªéŸ³åˆæˆéŒ¯èª¤: {e}")
        
        print("ğŸ”Š èªéŸ³åˆæˆç·šç¨‹åœæ­¢")
    
    def _playback_worker(self):
        """éŸ³é »æ’­æ”¾ç·šç¨‹"""
        print("ğŸ”ˆ éŸ³é »æ’­æ”¾ç·šç¨‹å•Ÿå‹•")
        
        while self.is_active or not self.synthesis_queue.empty():
            try:
                audio_file, capture_time = self.synthesis_queue.get(timeout=1)
                
                # æ’­æ”¾éŸ³é »
                pygame.mixer.music.load(audio_file)
                pygame.mixer.music.play()
                
                total_latency = time.time() - capture_time
                self.processing_times.append(total_latency)
                
                avg_latency = sum(self.processing_times) / len(self.processing_times)
                print(f"ğŸ”ˆ æ’­æ”¾é–‹å§‹ï¼Œç¸½å»¶é²: {total_latency:.2f}s, å¹³å‡: {avg_latency:.2f}s")
                
                if self.gui:
                    self.gui.update_latency(total_latency, avg_latency)
                
                # ç­‰å¾…æ’­æ”¾å®Œæˆ
                while pygame.mixer.music.get_busy():
                    time.sleep(0.1)
                    if self.should_stop:
                        pygame.mixer.music.stop()
                        break
                
                # æ¸…ç†æ–‡ä»¶
                try:
                    os.unlink(audio_file)
                except:
                    pass
                
                self.synthesis_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ éŸ³é »æ’­æ”¾éŒ¯èª¤: {e}")
        
        print("ğŸ”ˆ éŸ³é »æ’­æ”¾ç·šç¨‹åœæ­¢")

class RealtimeTranslatorGUI:
    def __init__(self):
        self.root = tk.Tk()
        self.root.title("âš¡ è¶…ä½å»¶é²å³æ™‚èªéŸ³ç¿»è­¯ç³»çµ±")
        self.root.geometry("1000x700")
        self.root.configure(bg='#1a1a1a')
        
        self.translator = RealtimeVoiceTranslator()
        self.translator.gui = self
        
        self.is_active = False
        self.create_widgets()
    
    def create_widgets(self):
        # æ¨™é¡Œ
        title_frame = tk.Frame(self.root, bg='#1a1a1a')
        title_frame.pack(pady=10)
        
        tk.Label(title_frame, text="âš¡ è¶…ä½å»¶é²å³æ™‚èªéŸ³ç¿»è­¯", 
                font=('Arial', 20, 'bold'), 
                bg='#1a1a1a', fg='#00ff00').pack()
        
        tk.Label(title_frame, text="ç›®æ¨™å»¶é² < 3ç§’", 
                font=('Arial', 12), 
                bg='#1a1a1a', fg='#ffff00').pack()
        
        # APIè¨­ç½®
        api_frame = tk.LabelFrame(self.root, text="APIè¨­ç½®", 
                                 bg='#2a2a2a', fg='white', font=('Arial', 12, 'bold'))
        api_frame.pack(fill='x', padx=10, pady=5)
        
        tk.Label(api_frame, text="Gemini API Key:", 
                bg='#2a2a2a', fg='white').grid(row=0, column=0, sticky='w', padx=5)
        
        self.api_entry = tk.Entry(api_frame, width=50, show='*')
        self.api_entry.grid(row=0, column=1, padx=5)
        
        tk.Button(api_frame, text="è¨­ç½®API", command=self.setup_api,
                 bg='#4a4a4a', fg='white').grid(row=0, column=2, padx=5)
        
        # èªè¨€è¨­ç½®
        lang_frame = tk.LabelFrame(self.root, text="èªè¨€è¨­ç½®", 
                                  bg='#2a2a2a', fg='white', font=('Arial', 12, 'bold'))
        lang_frame.pack(fill='x', padx=10, pady=5)
        
        tk.Label(lang_frame, text="åŸå§‹èªè¨€:", 
                bg='#2a2a2a', fg='white').grid(row=0, column=0, padx=5)
        
        self.source_var = tk.StringVar(value='zh')
        source_combo = ttk.Combobox(lang_frame, textvariable=self.source_var,
                                   values=list(self.translator.supported_languages.keys()),
                                   state='readonly', width=10)
        source_combo.grid(row=0, column=1, padx=5)
        
        tk.Label(lang_frame, text="ç›®æ¨™èªè¨€:", 
                bg='#2a2a2a', fg='white').grid(row=0, column=2, padx=5)
        
        self.target_var = tk.StringVar(value='en')
        target_combo = ttk.Combobox(lang_frame, textvariable=self.target_var,
                                   values=list(self.translator.supported_languages.keys()),
                                   state='readonly', width=10)
        target_combo.grid(row=0, column=3, padx=5)
        
        # æ§åˆ¶æŒ‰éˆ•
        control_frame = tk.Frame(self.root, bg='#1a1a1a')
        control_frame.pack(pady=10)
        
        tk.Button(control_frame, text="è¼‰å…¥æ¨¡å‹", command=self.load_models,
                 bg='#4a4a4a', fg='white', font=('Arial', 12)).pack(side='left', padx=5)
        
        tk.Button(control_frame, text="éŒ„è£½èªéŸ³", command=self.clone_voice,
                 bg='#4a4a4a', fg='white', font=('Arial', 12)).pack(side='left', padx=5)
        
        self.start_button = tk.Button(control_frame, text="é–‹å§‹ç¿»è­¯", command=self.toggle_translation,
                                     bg='#006600', fg='white', font=('Arial', 14, 'bold'))
        self.start_button.pack(side='left', padx=10)
        
        # ç‹€æ…‹é¡¯ç¤º
        status_frame = tk.Frame(self.root, bg='#1a1a1a')
        status_frame.pack(fill='x', padx=10)
        
        self.status_label = tk.Label(status_frame, text="ç³»çµ±å°±ç·’", 
                                    bg='#1a1a1a', fg='#00ff00', font=('Arial', 12))
        self.status_label.pack(side='left')
        
        self.latency_label = tk.Label(status_frame, text="å»¶é²: --", 
                                     bg='#1a1a1a', fg='#ffff00', font=('Arial', 12))
        self.latency_label.pack(side='right')
        
        # æ–‡å­—é¡¯ç¤ºå€åŸŸ
        text_frame = tk.Frame(self.root, bg='#1a1a1a')
        text_frame.pack(fill='both', expand=True, padx=10, pady=10)
        
        # åŸæ–‡é¡¯ç¤º
        tk.Label(text_frame, text="åŸæ–‡ï¼š", bg='#1a1a1a', fg='white', 
                font=('Arial', 12, 'bold')).pack(anchor='w')
        
        self.original_text = scrolledtext.ScrolledText(text_frame, height=8, 
                                                      bg='#2a2a2a', fg='white',
                                                      font=('Arial', 11))
        self.original_text.pack(fill='both', expand=True, pady=(0, 10))
        
        # ç¿»è­¯é¡¯ç¤º
        tk.Label(text_frame, text="ç¿»è­¯ï¼š", bg='#1a1a1a', fg='white', 
                font=('Arial', 12, 'bold')).pack(anchor='w')
        
        self.translated_text = scrolledtext.ScrolledText(text_frame, height=8, 
                                                        bg='#2a2a2a', fg='#00ff00',
                                                        font=('Arial', 11))
        self.translated_text.pack(fill='both', expand=True)
    
    def setup_api(self):
        api_key = self.api_entry.get().strip()
        if not api_key:
            messagebox.showerror("éŒ¯èª¤", "è«‹è¼¸å…¥API Key")
            return
        
        if self.translator.setup_api(api_key):
            messagebox.showinfo("æˆåŠŸ", "APIè¨­ç½®æˆåŠŸ")
            self.update_status("APIå·²è¨­ç½®")
        else:
            messagebox.showerror("éŒ¯èª¤", "APIè¨­ç½®å¤±æ•—")
    
    def load_models(self):
        self.update_status("è¼‰å…¥æ¨¡å‹ä¸­...")
        
        def load_in_thread():
            success = self.translator.load_models()
            self.root.after(0, lambda: self.model_loaded(success))
        
        threading.Thread(target=load_in_thread, daemon=True).start()
    
    def model_loaded(self, success):
        if success:
            self.update_status("æ¨¡å‹è¼‰å…¥å®Œæˆ")
            messagebox.showinfo("æˆåŠŸ", "æ¨¡å‹è¼‰å…¥å®Œæˆ")
        else:
            self.update_status("æ¨¡å‹è¼‰å…¥å¤±æ•—")
            messagebox.showerror("éŒ¯èª¤", "æ¨¡å‹è¼‰å…¥å¤±æ•—")
    
    def clone_voice(self):
        if not self.translator.tone_color_converter:
            messagebox.showerror("éŒ¯èª¤", "è«‹å…ˆè¼‰å…¥æ¨¡å‹")
            return
        
        self.update_status("èªéŸ³éŒ„è£½ä¸­...")
        
        def clone_in_thread():
            success = self.translator.clone_voice()
            self.root.after(0, lambda: self.voice_cloned(success))
        
        threading.Thread(target=clone_in_thread, daemon=True).start()
    
    def voice_cloned(self, success):
        if success:
            self.update_status("èªéŸ³å…‹éš†å®Œæˆ")
            messagebox.showinfo("æˆåŠŸ", "èªéŸ³å…‹éš†å®Œæˆ")
        else:
            self.update_status("èªéŸ³å…‹éš†å¤±æ•—")
            messagebox.showerror("éŒ¯èª¤", "èªéŸ³å…‹éš†å¤±æ•—")
    
    def toggle_translation(self):
        if not self.is_active:
            # æª¢æŸ¥æº–å‚™ç‹€æ…‹
            if not self.translator.model:
                messagebox.showerror("éŒ¯èª¤", "è«‹å…ˆè¨­ç½®API")
                return
            
            if not self.translator.tone_color_converter:
                messagebox.showerror("éŒ¯èª¤", "è«‹å…ˆè¼‰å…¥æ¨¡å‹")
                return
            
            if self.translator.target_se is None:
                messagebox.showerror("éŒ¯èª¤", "è«‹å…ˆéŒ„è£½èªéŸ³")
                return
            
            # è¨­ç½®èªè¨€
            self.translator.source_language = self.source_var.get()
            self.translator.target_language = self.target_var.get()
            
            # æ¸…ç©ºæ–‡å­—å€åŸŸ
            self.original_text.delete(1.0, tk.END)
            self.translated_text.delete(1.0, tk.END)
            
            # å•Ÿå‹•ç¿»è­¯
            if self.translator.start_realtime_translation():
                self.is_active = True
                self.start_button.config(text="åœæ­¢ç¿»è­¯", bg='#cc0000')
                self.update_status("å³æ™‚ç¿»è­¯å•Ÿå‹•")
        else:
            # åœæ­¢ç¿»è­¯
            self.translator.stop_realtime_translation()
            self.is_active = False
            self.start_button.config(text="é–‹å§‹ç¿»è­¯", bg='#006600')
            self.update_status("ç¿»è­¯å·²åœæ­¢")
    
    def update_status(self, message):
        self.status_label.config(text=message)
        self.root.update_idletasks()
    
    def update_latency(self, current, average):
        color = '#00ff00' if current < 3.0 else '#ffff00' if current < 5.0 else '#ff0000'
        self.latency_label.config(
            text=f"å»¶é²: {current:.1f}s (å¹³å‡: {average:.1f}s)",
            fg=color
        )
    
    def add_text(self, original, translated):
        timestamp = datetime.now().strftime("%H:%M:%S")
        
        self.original_text.insert(tk.END, f"[{timestamp}] {original}\n")
        self.original_text.see(tk.END)
        
        self.translated_text.insert(tk.END, f"[{timestamp}] {translated}\n")
        self.translated_text.see(tk.END)
    
    def run(self):
        self.root.protocol("WM_DELETE_WINDOW", self.on_closing)
        self.root.mainloop()
    
    def on_closing(self):
        if self.is_active:
            self.translator.stop_realtime_translation()
        self.root.destroy()

if __name__ == "__main__":
    app = RealtimeTranslatorGUI()
    app.run()